<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title>SAPHanaSR-ScaleOut</title>
	<meta name="generator" content="LibreOffice 5.1.5.2 (Linux)"/>
	<meta name="created" content="00:00:00"/>
	<meta name="changed" content="2016-10-26T19:30:49.453445647"/>
	<meta name="Content-Style" content="text/css"/>
	<!-- Creator     : groff version 1.22.2 -->
	<!-- CreationDate: Wed Oct 26 17:56:02 2016 -->
	<style type="text/css">
		h1 { text-align: center }
		h2.cjk { font-family: "WenQuanYi Micro Hei" }
		h2.ctl { font-family: "FreeSans" }
		h3.western { font-family: "Albany", sans-serif; font-size: 14pt }
		h3.cjk { font-family: "WenQuanYi Micro Hei"; font-size: 13pt }
		h3.ctl { font-family: "FreeSans"; font-size: 13pt }
	</style>
</head>
<body lang="de-DE" dir="ltr">
<h1 align="left">SAPHanaSR-ScaleOut</h1>
<p style="margin-bottom: 0in"><a href="#NAME">NAME</a><br/>
<a href="#DESCRIPTION">DESCRIPTION</a><br/>
<a href="#REQUIREMENTS">REQUIREMENTS</a><br/>
<a href="#BUGS">BUGS</a><br/>
<a href="#SEE ALSO">SEE
ALSO</a><br/>
<a href="#AUTHORS">AUTHORS</a><br/>
<a href="#COPYRIGHT">COPYRIGHT</a></p>
<hr/>

<h2 class="western"><a name="DESCRIPTION"></a>What is
SAPHanaSR-ScaleOut?</h2>
<p><b>Overview</b></p>
<p>The SAPHanaSR-ScaleOut package provides resource agents (RA) and
tools for setting up and managing automation of SAP HANA system
replication (SR) in scale-out setups. System replication will help to
replicate the database data from one site to another site in order to
compensate for database failures. With this mode of operation,
internal SAP HANA high-availability (HA) mechanisms and the Linux
cluster have to work together.</p>
<p>An HANA scale-out setup already is, to some degree, an HA cluster
on its own. The HANA is able to replace failing nodes with standby
nodes or to restart certain sub-systems on other nodes. As long as
the HANA landscape status is not &quot;ERROR&quot; the Linux cluster
will not act. The main purpose of the Linux cluster is to handle the
take-over to the other site. Only if the HANA landscape status
indicates that HANA can not recover from the failure and the
replication is in sync, then Linux will act. As an exception, the
Linux cluster will react if HANA moves the master nameserver role to
another candidate. SAPHanaController is also able to restart former
failed worker nodes as standby. In addition to the SAPHanaTopology
RA, the SAPHanaSR-ScaleOut solution uses a &quot;HA/DR providers&quot;
API provided by HANA to get informed about the current state of the
system replication.</p>
<p>Note: To automate SAP HANA SR in scale-up setups, please use the
package SAPHanaSR.</p>
<p><b>Scenarios</b></p>
<p>In order to illustrate the meaning of the above overview, some
important situations are described below. This is not a complete
description of all situations.</p>
<p>1. <b>Local start of standby node</b> <br/>
The Linux cluster will
react if HANA moves any worker node (including the master nameserver
role) to another candidate. If the failed node or instance is
available in the cluster and switched to HANA standby role, the Linux
cluster will restart the SAP HANA local framework so this node could
be used for future failovers. This is one exception from the general
rule, that the Linux cluster does nothing as long as the HANA
landscape status is not &quot;ERROR&quot;.</p>
<p>2. <b>Prevention against dual-primary</b> <br/>
A primary
absolutely must never be started, if the cluster does not know
anything about the other site. On initial cluster start, the cluster
needs to detect a valid HANA system replication setup, including
system replication status (SOK) and last primary timestamp (LPT).
This is neccessary to ensure data integrity.</p>
<p>The rational behind this is shown in the following scenario: <br/>
1.
site_A is primary, site_B is secondary - they are in sync. <br/>
2.
site_A crashes (remember the HANA ist still marked primary). <br/>
3.
site_B does the take-over and runs now as new primary. <br/>
4. DATA
GETS CHANGED ON NODE2 BY PRODUCTION <br/>
5. The admin also stops the
cluster on site_B (we have two HANAs both <br/>
internally marked
down and primary now). <br/>
6. What, if the admin would now restart
the cluster on site_A? <br/>
6.1 site_A would take its own CIB after
waiting for the initial fencing <br/>
time for site_B. <br/>
6.2 It
would &quot;see&quot; its own (cold) primary and the fact that there
was a <br/>
secondary. <br/>
6.3 It would start the HANA from point
of time of step 1.-&gt;2. (the crash), <br/>
so all data changed
inbetween would be lost. <br/>
This is why the Linux cluster needs to
enforce a restart inhibit.</p>
<p>There are two options to get back both, SAP HANA SR and the Linux
cluster, into a fully functional state: <br/>
a) the admin starts
both nodes again <br/>
b) In the situation where the site_B is still
down, the admin starts the <br/>
primary on site_A manually. <br/>
The
Linux cluster will follow this administrative decision. In both cases
the administrator should register and start a secondary as soon as
posible. This avoids a full log partition with consequence of a
DATABASE STUCK.</p>
<p>3. <b>Automatic registration as secondary after site failure and
takeover</b> <br/>
The cluster can be configured to register a former
primary database automatically as secondary. If this option is set,
the resource agent will register a former primary database as
secondary during cluster/resource start.</p>
<p>4. <b>Site take-over not preferred over local re-start</b>
<br/>
SAPHanaSR-ScaleOut allows to configure, if you prefer to
takeover to the secondary after the primary landscape fails. The
alternative is to restart the primary landscape, if it fails and only
to takeover when no local restart is possible anymore. This can be
tuned by SAPHanaController(7) parameters. <br/>
The current
implementation only allows to takeover in case the landscape status
reports 1 (ERROR). The cluster will not takeover, when the SAP HANA
still tries to repair a local failure.</p>
<p>5. <b>Recovering from failure of master nameserver</b> <br/>
If
the master nameserver of an HANA database system fails, the HANA will
start the nameserver on another node. Therefore usually up to two
nodes are configured as additional nameserver candidates. At least
one of them should be a standby node to optimize failover time. The
Linux cluster will detect the change and move the IP address to the
new active master nameserver.</p>
<p><b>Implementation</b></p>
<p>The two HANA database systems (primary and secondary site) are
managed by the same single Linux cluster. The maximum number of nodes
in that single Linux cluster is given by the Linux cluster limit. An
odd number of nodes is needed to handle split-brain situations
automatically in stretched clusters.</p>
<p>The HANA consists of two sites with same number of nodes each.
There are no HANA nodes outside the Linux cluster. An additional
Linux cluster node is used as majority maker for split-brain
situations. This dedicated node does not need to have HANA installed
and must not run any SAP HANA resources for the same SID.</p>
<p>A common STONITH mechanism is set up for all nodes accross all the
sites.</p>
<p>Since the IP address of the primary HANA database system is
managed by the cluster, only that single IP address is needed to
access any nameserver candidate.</p>
<p><b>Best Practice</b></p>
<p><b>*</b> Use two independent corosync rings, at least one of them
on bonded network. Resulting in at least three physical links.
Unicast is preferred.</p>
<p><b>*</b> Use Stonith Block Device (SBD), shared LUNs across all
nodes on all (three) sites. Of course, together with hardware
watchdog.</p>
<p><b>*</b> Align all timeouts in the Linux cluster with the timeouts
of the underlying storage and multipathing.</p>
<p><b>*</b> Check the installation of OS and Linux cluster on all
nodes before doing any functional tests.</p>
<p><b>*</b> Carefully define, perform, and document tests for all
scenarios that should be covered.</p>
<p><b>*</b> Test HANA HA and SR features without Linux cluster before
doing the overall cluster tests.</p>
<p><b>*</b> Test basic Linux cluster features without HANA before
doing the overall cluster tests.</p>
<p><b>*</b> Be patient. For detecting the overall HANA status, the
Linux cluster needs a certain amount of time, depending on the HANA
and the configured intervalls and timeouts.</p>
<p><b>*</b> Before doing anything, always check for the Linux
clusterâ€™s idle status, the HANA landscape status, and the HANA SR
status.</p>
<h2 class="western"><a name="REQUIREMENTS"></a>REQUIREMENTS 
</h2>
<p>For the current version of the package SAPHanaSR-ScaleOut, the
support is limited to the following scenarios and parameters:</p>
<p>1. HANA scale-out cluster with system replication. The two HANA
database systems (primary and secondary site) are managed by the same
single Linux cluster. The maximum number of nodes in that single
Linux cluster is given by the Linux cluster limit. An odd number of
nodes is needed to handle split-brain situations automatically. A
dedicated cluster node might be used as majority maker.</p>
<p>2. Technical users and groups such as sidadm are defined locally
in the Linux system.</p>
<p>3. Strict time synchronization between the cluster nodes, f.e.
NTP.</p>
<p>4. For scale-out there is no other SAP HANA system (like QA) on
the replicating node which needs to be stopped during take-over.</p>
<p>5. Only one system replication for the SAP HANA database.</p>
<p>6. Both SAP HANA database systems have the same SAP Identifier
(SID) and Instance Number.</p>
<p>7. Besides SAP HANA you need SAP hostagent to be installed and
started on your system.</p>
<p>8. Automated start of SAP HANA database systems during system boot
must be switched off.</p>
<p>9. For scale-out, the current resource agent supports SAP HANA in
system replication beginning with HANA version 1.0 SPS 11 patch level
112.02. Older versions do not provide the srHook method
srConnectionChanged().</p>
<p>10. For scale-out, if the shared storage is implemented with
another cluster, that one does not interfere with the Linux cluster.
All three clusters (HANA, storage, Linux) have to be aligned.</p>
<h2 class="western"><a name="SEE ALSO"></a>Further Documentation</h2>
<h3 class="western">Web Sites</h3>
<p>https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices.html
, <br/>
https://www.suse.com/releasenotes/ ,
<br/>
https://www.susecon.com/doc/2015/sessions/TUT19921.pdf ,
<br/>
https://www.susecon.com/doc/2016/sessions/TUT90846.pdf ,
<br/>
http://scn.sap.com/community/hana-in-memory/blog/2014/04/04/fail-safe-operation-of-sap-hana-suse-extends-its-high-availability-solution
, <br/>
http://scn.sap.com/docs/DOC-60334 ,
<br/>
http://scn.sap.com/community/hana-in-memory/blog/2015/12/14/sap-hana-sps-11-whats-new-ha-and-dr--by-the-sap-hana-academy</p>
<h3 class="western">Man Pages</h3>
<p><b>ocf_suse_SAPHanaTopology</b>(7) , <b>ocf_suse_SAPHanaController</b>(7)
, <b>ocf_heartbeat_IPaddr2</b>(7) , <b>SAPHanaSR-monitor</b>(8) ,
<b>SAPHanaSR-showAttr</b>(8) , <b>SAPHanaSR.py</b>(7) , <b>ntp.conf</b>(5)
, <b>stonith</b>(8) , <b>sbd</b>(8) , <b>stonith_sbd</b>(7) , <b>crm</b>(8)
, <b>corosync.conf</b>(5) , <b>crm_no_quorum_policy</b>(7) ,
<b>cs_precheck_for_hana</b>(8) , <b>cs_add_watchdog_to_initrd</b>(8)
<br/>
<br/>
<br/>

</p>
</body>
</html>