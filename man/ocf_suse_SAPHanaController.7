.\" Version: 0.162.3
.\"
.TH ocf_suse_SAPHanaController 7 "22 Dec 2016" "" "OCF resource agents"
.\"
.SH NAME
SAPHanaController \- Manages take-over between two SAP HANA databases with system replication.
.PP
.\"
.SH SYNOPSYS
\fBSAPHanaController\fP [start | stop | status | monitor | promote | demote | notify | meta\-data | validate\-all | methods | usage ]
.PP
.\"
.SH DESCRIPTION

This SAPHanaController is a resource agent for SAP HANA databases in 
scale-out setups. It manages take-over for a SAP HANA database with 
system replication in an OCF master/slave configuration.
.PP
System replication will help to replicate the database data from one site to
another site in order to compensate for database failures. With this mode of
operation, internal SAP HANA high-availability (HA) mechanisms and the Linux
cluster have to work together.
.PP
An HANA scale-out setup already is, to some degree, an HA cluster on its own.
The HANA is able to replace failing nodes with standby nodes or to restart
certain sub-systems on other nodes. As long as the HANA landscape status is
not "ERROR" the Linux cluster will not act. The main purpose of the Linux
cluster is to handle the take-over to the other site. Only if the HANA
landscape status indicates that HANA can not recover from the failure and the
replication is in sync, then Linux will act. As an exception, the Linux cluster
will react if HANA moves the master nameserver role to another candidate. 
SAPHanaController is also able to restart former failed worker nodes as standby.
In addition to the SAPHanaTopology RA, the SAPHanaSR-ScaleOut solution uses a
"HA/DR providers" API provided by HANA to get informed about the current state of the 
system replication.
.PP
On initial cluster start, the cluster needs to detect a valid HANA system
replication setup, including system replication status (SOK) and last primary
timestamp (LPT). This is neccessary to ensure data integrity.
.PP
The SAPHanaController resource agent (RA) performs the actual check of the
SAP HANA database instances and is configured as a master/slave resource.
The Linux cluster resource's master status will follow the HANA's active master
name server role. Resources (f.e. IP address) that need to run on the same node 
with that HANA role should be co-located with the master target.
.\" TODO sketch to show relation between roles
.\" Linux Cluster (res) Linux Cluster (m/s)   HANA      
.\" Follow    <=>       Master     ==         Primary Name Server    
.PP
Managing the two SAP HANA instances in scale-out setup means that the resource
agent controls the start/stop of the instances. In addition the resource agent
is able to monitor the SAP HANA databases on landscape host configuration level.
For this monitoring the resource agent relies on interfaces provided by SAP.
A third task of the resource agent is to also check the synchronisation status
of the two SAP HANA databases. If the synchronisation is not "SOK", then the
cluster avoids to take-over to the secondary side, if the primary fails. This
is to improve the data consistency.
.PP
The resource agent uses the following five interfaces provided by SAP:
.PP
1. \fBsapcontrol/sapstartsrv\fP
.br
The interface sapcontrol/sapstartsrv is used to start/stop a HANA
database instance/system. SAPHanaController also checks via this interface
the instance status to allow a start of HANA standby nodes.
.PP
2. \fBlandscapeHostConfiguration\fP
.br
The interface is used to monitor an entire HANA system. The python script
is named landscapeHostConfiguration.py.
landscapeHostConfiguration.py has some detailed output about HANA system status
and node roles. For our monitor the overall status is relevant.
This overall status is reported by the returncode of the script:
0: Internal Fatal, 1: ERROR, 2: WARNING, 3: INFO, 4: OK
The SAPHanaController resource agent will interpret returncode 0 as FATAL,
1 as not-running (or ERROR) and returncodes 2+3+4 as RUNNING.
.PP
3. \fBhdbnsutil\fP
.br
The interface hdbnsutil is used to check the "topology" of the system
replication as well as the current configuration (primary/secondary) of a
SAP HANA database instance. A second task of the interface is the posibility
to run a system replication take-over (sr_takeover) or to register a former
primary to a newer one (sr_register). In scale-out setups, hdbnsutil sometimes
might take some time.
.PP
4. \fBsystemReplicationStatus\fP
.br
The interface systemReplicationStatus is a special query to check the HANA 
system replication status via a python library. 
This interface exists since SAP HANA SPS9.
.PP
5. \fBsaphostctrl\fP
.br
The interface saphostctrl uses the function ListInstances to figure out the
virtual host name of the SAP HANA instance. This is typically the hostname
used during the HANA installation.
.PP
To make configuring the cluster as simple as possible, the additional
SAPHanaTopology resource agent runs on all nodes of the Linux cluster and
gathers information about the statuses and configurations of SAP HANA system
replication. The SAPHanaTopology RA is designed as a normal (stateless) clone.
.PP  
Please see also the REQUIREMENTS section below.
.RE
.PP
.\"
.SH SUPPORTED PARAMETERS
This resource agent supports the following parameters:
.PP
\fBSID\fR
.RS 4
SAP System Identifier. Has to be same on both instances. 
Example "SID=SLE".
.RE
.PP
\fBInstanceNumber\fR
.RS 4
Number of the SAP HANA database.
For system replication also Instance Number+1 is blocked. 
Example "InstanceNumber=00".
.RE
.PP
\fBDIR_EXECUTABLE\fR
.RS 4
The full qualified path where to find sapstartsrv and sapcontrol.
Specify this parameter, if you have changed the SAP kernel directory
location after the default SAP installation.
.br
Optional, well known directories will be searched by default.
.RE
.PP
\fBDIR_PROFILE\fR
.RS 4
The full qualified path where to find the SAP START profile.
Specify this parameter, if you have changed the SAP profile directory
location after the default SAP installation.
.br
Optional, well known directories will be searched by default.
.RE
.PP
\fBINSTANCE_PROFILE\fR
.RS 4
The name of the SAP HANA instance profile. Specify this parameter,
if you have changed the name of the SAP HANA instance profile
after the default SAP installation.
Normally you do not need to set this parameter.
.br
Optional, well known directories will be searched by default.
.RE 
.PP
\fBPREFER_SITE_TAKEOVER\fR
.RS 4
Defines whether RA should prefer to take-over to the slave database
instead of restarting master locally. However a take-over will only
be triggered, if the SAP HANA landscape status is on "ERROR".
Example: "PREFER_SITE_TAKEOVER=true".
.br
Optional. Default value: false\&.
.RE
.PP
\fBDUPLICATE_PRIMARY_TIMEOUT\fR
.RS 4
Time difference needed between two primary time stamps (LPTs), in case
a dual-primary situation occurs. If the difference between both node's
last primary time stamps is less than DUPLICATE_PRIMARY_TIMESTAMP,
then the cluster holds one or both instances in a "WAITING" status.
This is to give an admin the chance to react on a failover.
Note: How the cluster proceeds after the DUPLICATE_PRIMARY_TIMESTAMP
has passed, depends on the parameter AUTOMATED_REGISTER.
See also the examples section below.
.br
Optional. Default value: 7200\&.
.RE
.PP
\fBAUTOMATED_REGISTER\fR
.RS 4
Defines whether a former primary database should be registered
automatically by the resource agent during cluster/resource start,
if the DUPLICATE_PRIMARY_TIMEOUT condition is met.
Example: "AUTOMATED_REGISTER=true".
.br
Default value: false\&.
.RE
.PP
.\"
.SH SUPPORTED PROPERTIES
.br
\fBhana_${sid}_glob_filter\fR
.RS 4
Global cluster property \fBhana_${sid}_glob_filter\fR .
This property should only be set if requested by support engineers.
The default is sufficient for normal operation.
.RE
.PP
.\"
.SH SUPPORTED ACTIONS
This resource agent supports the following actions (operations):
.PP
\fBstart\fR
.RS 4
Starts the HANA instance or bring the "clone instance" to a WAITING status.
Suggested minimum timeout: 3600\&.
.RE
.PP
\fBstop\fR
.RS 4
Stops the HANA instance. 
Suggested minimum timeout: 3600\&.
.RE
.PP
\fBpromote\fR
.RS 4
Either runs a take-nover for a secondary or a just-nothing for a primary.
Suggested minimum timeout: 900\&.
.RE
.PP
\fBdemote\fR
.RS 4
Nearly does nothing and just marks the instance as demoted.
Suggested minimum timeout: 320\&.
.RE
.PP
\fBnotify\fR
.RS 4
Always returns SUCCESS.
Suggested minimum timeout: 10\&.
.RE
.PP
\fBstatus\fR
.RS 4
Reports whether the HANA instance is running.
Suggested minimum timeout: 60\&.
.RE
.PP
\fBmonitor (Master role)\fR
.RS 4
Reports whether the HANA database seems to be working in
master/slave. It also needs to check the system replication status.
Suggested minimum timeout: 700\&.
Suggested interval: 60\&.
.RE
.PP
\fBmonitor (Slave role)\fR
.RS 4
Reports whether the HANA database seems to be working in
master/slave. It also needs to check the system replication status.
Suggested minimum timeout: 700\&.
Suggested interval: 61\&.
.RE
.PP
\fBvalidate\-all\fR
.RS 4
Reports whether the parameters are valid.
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmeta\-data\fR
.RS 4
Retrieves resource agent metadata (internal use only).
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmethods\fR
.RS 4
Suggested minimum timeout: 5\&.
.RE
.PP
.\"
.SH RETURN CODES
The return codes are defined by the OCF cluster framework.
Please refer to the OCF definition on the website mentioned below. 
.br
In addition, log entries are written, which can be scanned by using a
pattern like "SAPHanaController.*RA.*rc=[1-7,9]" for errors.
Regular operations might be found with "SAPHanaController.*RA.*rc=0".
.PP
.\"
.SH EXAMPLES
.\" .PP
.\" * This is an example configuration for a SAPHanaController resource for HANA scale-up.
.\" .br
.\" In addition, a SAPHanaTopology resource is needed to make this work.
.\" .RE
.\" .PP
.\" .RS 4
.\" primitive rsc_SAPHanaController_SLE_HDB00 ocf:suse:SAPHanaController \\
.\" .br
.\" operations $id="rsc_sap_SLE_HDB00-operations" \\
.\" .br
.\" op start interval="0" timeout="3600" \\
.\" .br
.\" op stop interval="0" timeout="3600" \\
.\" .br
.\" op promote interval="0" timeout="3600" \\
.\" .br
.\" op monitor interval="60" role="Master" timeout="700" \\
.\" .br
.\" op monitor interval="61" role="Slave" timeout="700" \\
.\" .br
.\" params SID="SLE" InstanceNumber="00" PREFER_SITE_TAKEOVER="true" \\
.\" .br
.\" DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"
.\" .PP
.\" ms msl_SAPHanaController_SLE_HDB00 rsc_SAPHanaController_SLE_HDB00 \\
.\" .br
.\" clone-max="2" clone-node-max="1"
.\" .RE
.PP
* Below is an example configuration for a SAPHanaTopology resource for
HANA scale-out.
.br
The HANA consists of two sites with five nodes each. An additional
cluster node is used as majority maker for split-brain situations.
In addition, a SAPHanaController resource is needed to make this work.
.RE
.PP
.RS 4
primitive rsc_SAPHanaCon_SLE_HDB00 ocf:suse:SAPHanaController \\
.br
op start interval="0" timeout="3600" \\
.br
op stop interval="0" timeout="3600" \\
.br
op promote interval="0" timeout="3600" \\
.br
op monitor interval="60" role="Master" timeout="700" \\
.br
op monitor interval="61" role="Slave" timeout="700" \\
.br
params SID="SLE" InstanceNumber="00" PREFER_SITE_TAKEOVER="true" \\
.br
DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="true"
.PP
ms msl_SAPHanaCon_SLE_HDB00 rsc_SAPHanaCon_SLE_HDB00 \\
.br
master-node-max="1" master-max="1" clone-node-max="1" interleave="true"
.PP
location SAPHanaCon_not_on_majority_maker msl_SAPHanaCon_HAE_HDB00 -inf: vm-majority
.RE
.PP
* The following shows the filter for log messages set to the defaults.
.br
This property should only be set if requested by support engineers.
The default is sufficient for normal operation.
.RE
.PP
.RS 4
property $id="SAPHanaSR" \\
.br
hana_SLE_glob_filter="ra-act-dec-lpa"
.RE
.TP
* Search for log entries of the resource agent, show errors only:
.PP
.RS 4
# grep "SAPHanaController.*RA.*rc=[1-7,9]" /var/log/messages
.\" TODO: output
.RE
.PP
* Check for working NTP service:
.PP
.RS 4
# ntpq -p
.\" TODO:
.\"     remote           refid      st t when poll reach   delay   offset  jitter
.\"==============================================================================
.\" LOCAL(0)        .LOCL.          10 l   29   64  177    0.000    0.000   0.001
.\"*129.70.132.32   129.70.130.71    2 u   25   64  177   24.844  -25796.   9.929
.\"+141.30.228.4    5.9.110.236      3 u   32   64   77   37.789  -25795.   4.910
.RE
.PP
* Use of DUPLICATE_PRIMARY_TIMEOUT and Last Primary Timestamp (LPT) in case
the primary node has been crashed completely.

Typically on each side where the RA detects a running primary a time stamp is
written to the node's attributes (last primary seen at time: lpt).
If the timestamps ("last primary seen at") differ less than the
DUPLICATE_PRIMARY_TIMEOUT than the RA could not automatically decide which
of the two primaries is the better one.

1. nodeA is primary and has a current time stamp, nodeB is secondary and has
a secondary marker set:
.br
nodeA: 1479201695
.br
nodeB: 30

2. Now nodeA crashes and nodeB takes over:
.br
(nodeA: 1479201695)
.br
nodeB: 1479201700

3. A bit later nodeA comes back into the cluster:
.br
nodeA: 1479201695
.br
nodeB: 1479202000
.br
You see while nodeA keeps its primary down the old timestamp is kept.
NodeB increases its timestamp on each monitor run.

4. After some more time (depending on the parameter DUPLICATE_PRIMARY_TIMEOUT)
.br
nodeA: 1479201695
.br
nodeB: 1479208895
.br
Now the time stamps differ >= DUPLICATE_PRIMARY_TIMEOUT. The algorithm defines
nodeA now as "the looser" and depending on the AUTOMATED_REGISTER the nodeA
will become the secondary.

5. NodeA would be registered:
.br
nodeA: 10
.br
nodeB: 1479208900

6. Some time later the secondary gets into sync
.br
nodeA: 30
.br
nodeB: 1479209100
.RE
.PP
* Use of DUPLICATE_PRIMARY_TIMEOUT and Last Primary Timestamp (LPT) in case
the the database on primary node has been crashed, but the node is still alive.

Typically on each side where the RA detects a running primary a time stamp is
written to the node's attributes (last primary seen at time: lpt).
If the timestamps ("last primary seen at") differ less than the
DUPLICATE_PRIMARY_TIMEOUT than the RA could not automatically decide which
of the two primaries is the better one.

1. nodeA is primary and has a current time stamp, nodeB is secondary and has
a secondary marker set:
.br
nodeA: 1479201695
.br
nodeB: 30

2. Now HANA on nodeA crashes and nodeB takes over:
.br
nodeA: 1479201695
.br
nodeB: 1479201700

3. As the cluster could be sure to properly stopped the HANA instance at nodeA
it *immediately* marks the old primary to be a register candidate,
if AUTOMATED_REGISTER is true:
.br
nodeA: 10
.br
nodeB: 1479201760

4. Depending on the AUTOMATED_REGISTER parameter the RA will also immediately
regisiter the former primary to become the new secondary:
.br
nodeA: 10
.br
nodeB: 1479201820

5. And after a while the secondary gets in sync
.br
nodeA: 30
.br
nodeB: 1479202132
.RE
.PP
.\"
.SH FILES
.TP
/usr/lib/ocf/resource.d/suse/SAPHanaController
    the resource agent
.TP
/usr/lib/ocf/resource.d/suse/SAPHanaTopology
    the also needed topology resource agent
.TP
/usr/sap/$SID/$InstanceName/exe
    default path for DIR_EXECUTABLE
.TP
/usr/sap/$SID/SYS/profile
    default path for DIR_PROFILE
.\"
.\" TODO: INSTANCE_PROFILE
.PP
.\"
.SH REQUIREMENTS
For the current version of the SAPHanaController resource agent that comes with
the software package SAPHanaSR-ScaleOut, the support is limited to the
scenarios and parameters described in the respective manual page
SAPHanaSR-ScaleOut(7).
.PP
.\"
.SH SEE ALSO
\fBocf_suse_SAPHanaTopology\fP(7) , \fBocf_heartbeat_IPaddr2\fP(8) , \fBSAPHanaSR-monitor\fP(8) ,
\fBSAPHanaSR-showAttr\fP(8) , \fBSAPHanaSR-ScaleOut\fP(7) ,
\fBntp.conf\fP(5) , \fBstonith\fP(8)
.br
https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices.html ,
.br
http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-ocf-return-codes.html ,
.br
http://scn.sap.com/community/hana-in-memory/blog/2014/04/04/fail-safe-operation-of-sap-hana-suse-extends-its-high-availability-solution ,
.br
http://www.saphana.com/docs/DOC-2775 ,
.br
http://scn.sap.com/docs/DOC-60334 ,
.br
http://scn.sap.com/docs/DOC-65899
.PP
.\"
.SH AUTHORS
F.Herschel, L.Pinne.
.PP
.\"
.SH COPYRIGHT
(c) 2014 SUSE Linux Products GmbH, Germany.
.br
(c) 2015-2016 SUSE Linux GmbH, Germany.
.br
The resource agent SAPHanaController comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
